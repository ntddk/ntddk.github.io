<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>ニューラルネットと脳の違いが知りたくて | 一生あとで読んでろ</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="読書メモ． はじめに　人間の脳を模したニューラルネットの手法，深層学習ディープラーニングがめざましい成果を挙げている–といった謳い文句をよく目にする．だが機械学習の専門書を紐解いても出てくるのはロジスティック回帰のお化けばかり．　われらがPRMLのニューラルネットを扱う章にはこうある．  しかしながら，パターン認識という実際的な応用の観点からは，生物学的な現実性などは全く不要な制約である．">
<meta name="keywords" content="machine learning">
<meta property="og:type" content="article">
<meta property="og:title" content="ニューラルネットと脳の違いが知りたくて">
<meta property="og:url" content="http://ntddk.github.io/2016/03/21/book-guide-for-computational-neuroscience/index.html">
<meta property="og:site_name" content="一生あとで読んでろ">
<meta property="og:description" content="読書メモ． はじめに　人間の脳を模したニューラルネットの手法，深層学習ディープラーニングがめざましい成果を挙げている–といった謳い文句をよく目にする．だが機械学習の専門書を紐解いても出てくるのはロジスティック回帰のお化けばかり．　われらがPRMLのニューラルネットを扱う章にはこうある．  しかしながら，パターン認識という実際的な応用の観点からは，生物学的な現実性などは全く不要な制約である．">
<meta property="og:updated_time" content="2017-12-14T07:40:23.725Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="ニューラルネットと脳の違いが知りたくて">
<meta name="twitter:description" content="読書メモ． はじめに　人間の脳を模したニューラルネットの手法，深層学習ディープラーニングがめざましい成果を挙げている–といった謳い文句をよく目にする．だが機械学習の専門書を紐解いても出てくるのはロジスティック回帰のお化けばかり．　われらがPRMLのニューラルネットを扱う章にはこうある．  しかしながら，パターン認識という実際的な応用の観点からは，生物学的な現実性などは全く不要な制約である．">
  
  
  <link href="//fonts.googleapis.com/css?family=Ubuntu" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
  
</head>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">一生あとで読んでろ</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">技術ブログ</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="sub-nav">
        
          <a id="nav-home-icon" class="nav-icon" href="/"></a>
        
          <a id="nav-about-icon" class="nav-icon" href="/about"></a>
        
        
      </nav>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-book-guide-for-computational-neuroscience" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      ニューラルネットと脳の違いが知りたくて
    </h1>
  

      </header>
    
    <time class="article-date" datetime="2016-03-20T15:30:00.000Z" itemprop="datePublished">03-21-2016</time>
    
  </div>
  <div class="article-inner">
    <div class="article-entry" itemprop="articleBody">
      
        <p>　読書メモ．</p>
<h1 id="はじめに"><a href="#はじめに" class="headerlink" title="はじめに"></a>はじめに</h1><p>　人間の脳を模したニューラルネットの手法，<ruby>深層学習<rt>ディープラーニング</rt></ruby>がめざましい成果を挙げている–といった謳い文句をよく目にする．だが機械学習の専門書を紐解いても出てくるのはロジスティック回帰のお化けばかり．<br>　われらがPRMLのニューラルネットを扱う章にはこうある．</p>
<blockquote>
<p>しかしながら，パターン認識という実際的な応用の観点からは，生物学的な現実性などは全く不要な制約である．<br>　　　　　　　　　　　　　　　　　　　　　　　　– C.M.ビショップ『<a href="http://www.amazon.co.jp/dp/4621061224" target="_blank" rel="external">パターン認識と機械学習 上</a>』 (p.226)</p>
</blockquote>
<p>　では，機械学習の文脈で言うところのニューラルネットと脳はどれほど異なっているのだろうか？</p>
<h1 id="ニューラルネットと脳の違い"><a href="#ニューラルネットと脳の違い" class="headerlink" title="ニューラルネットと脳の違い"></a>ニューラルネットと脳の違い</h1><p>　結論から言えば全然違うわけだが，ざっくり以下の三点から整理できる（と思う）：</p>
<ul>
<li>ニューロンのモデル</li>
<li>ネットワーク構造</li>
<li>微分計算の手法</li>
</ul>
<h2 id="ニューロンのモデル"><a href="#ニューロンのモデル" class="headerlink" title="ニューロンのモデル"></a>ニューロンのモデル</h2><p>　現在広く普及している多層パーセプトロンは，単純な差分方程式であるMcCulloch-Pittsモデルをベースに，層を重ねたとき微分できるような活性化関数を組み合わせている．だがそれ以外にも膜電位が変動するメカニズムを考慮した：</p>
<ul>
<li>Hodgkin-Huxleyモデル</li>
<li>FitzHugh-Nagumoモデル</li>
<li>Izhikevichモデル</li>
</ul>
<p>などが提案されている–というようなことは機械学習の道具としてニューラルネットを扱っている本にはあまり書かれていない．ひとまず手元にあった：</p>
<ul>
<li>はじパタこと『<a href="http://www.amazon.co.jp/dp/4627849710/" target="_blank" rel="external">はじめてのパターン認識</a>』</li>
<li>わかパタこと『<a href="http://www.amazon.co.jp/dp/4274131491/" target="_blank" rel="external">わかりやすいパターン認識</a>』</li>
<li>『<a href="http://www.amazon.co.jp/dp/B016Q22IX2/" target="_blank" rel="external">ITエンジニアのための機械学習理論入門</a>』</li>
<li>PRMLこと『<a href="http://www.amazon.co.jp/dp/4621061224/" target="_blank" rel="external">パターン認識と機械学習</a>』</li>
<li>青色の『<a href="http://www.amazon.co.jp/dp/B018K6C99A/" target="_blank" rel="external">深層学習</a>』</li>
<li>紫色の『<a href="http://www.amazon.co.jp/dp/B01B768QJW/" target="_blank" rel="external">深層学習</a>』</li>
</ul>
<p>はモデル名に言及しておらず，McCulloch-Pittsモデルを紹介していたのは：</p>
<ul>
<li>『<a href="http://www.amazon.co.jp/dp/4274218023/" target="_blank" rel="external">進化計算と深層学習</a>』</li>
<li>『<a href="http://www.amazon.co.jp/dp/406153825X" target="_blank" rel="external">イラストで学ぶディープラーニング</a>』</li>
</ul>
<p>だけだった．<br>　ちなみにこの中だと機械学習マジで何もわからんって人はまずサンプルコードを動かしながら『<a href="http://www.amazon.co.jp/dp/B016Q22IX2/" target="_blank" rel="external">ITエンジニアのための機械学習理論入門</a>』を読むとよい．深層学習については<a href="http://www.amazon.co.jp/dp/B01B768QJW/" target="_blank" rel="external">紫色の本</a>が好み．<a href="http://www.amazon.co.jp/dp/B018K6C99A/" target="_blank" rel="external">青色の本</a>は数式が追いにくい（どっちもどっちだが）上，深層学習以前と以後でどのような変遷があったのか不明瞭で，『<a href="http://www.amazon.co.jp/dp/4274218023/" target="_blank" rel="external">進化計算と深層学習</a>』は概論のみ．『<a href="http://www.amazon.co.jp/dp/406153825X" target="_blank" rel="external">イラストで学ぶディープラーニング</a>』は読みやすく，汎化性能を向上させる方法も載っていて実践的ではあるが，RNNに触れられていないのが惜しい．<br>　さて，それではニューロンのモデルが複雑であれば脳に近いのかというと，どうやらそういうわけでもないらしい．たとえば複雑かつイオンコンダクタンスの挙動が緻密なHodgkin-Huxleyモデルは，ヤリイカの軸索のニューロンをモデル化したものだが，これはヒトの大脳皮質とはタイプが異なるようだ．<br>　しかし，McCulloch-Pittsモデルには，STDP (Spike Timing Dependent Plasticity, スパイク時刻依存シナプス可塑性) を表現できていないという問題点がある．STDPはニューロンの発火タイミングに応じて重みを更新する規則で，脳はこれにより時系列を学習しているとされる．<br>　こういったことを把握するためにいくつか神経科学の本を読んでみた．だいたいの書籍がまず最初にニューロンのモデルを紹介し，それからニューロンの同期に章を割き，つづいて脳の任意の部位の詳説という体裁をとっている．</p>
<ul>
<li>『<a href="http://www.amazon.co.jp/dp/4130643010/" target="_blank" rel="external">脳の計算論</a>』</li>
<li>『<a href="http://www.amazon.co.jp/dp/4130623044/" target="_blank" rel="external">理工学系からの脳科学入門</a>』</li>
<li>『<a href="http://www.amazon.co.jp/dp/B006YKU67M/" target="_blank" rel="external">臨時別冊数理科学 SGCライブラリ 60 「計算神経科学への招待」脳の学習機構の理解を目指して 2007年 12月号</a>』</li>
<li>『<a href="http://www.amazon.co.jp/dp/478281514X/" target="_blank" rel="external">脳の計算理論</a>』</li>
</ul>
<p>など．全部読んだ感想としては『<a href="http://www.amazon.co.jp/dp/4130643010/" target="_blank" rel="external">脳の計算論</a>』が最も明快かつ簡潔で，STDPにも詳しく，この記事で触れている範囲のことはほぼカバーしている．<br>　Izhikevichモデルの考案者も『<a href="http://www.izhikevich.org/publications/dsn/index.htm" target="_blank" rel="external">Dynamical Systems in Neuroscience: The Geometry of Excitability and Bursting</a>』という本を書いているのだが，これは難しそう．<br>　本腰を入れて学ぶには「<a href="http://gaya.jp/research/LTP-papers.htm" target="_blank" rel="external">シナプス可塑性の初心者へ　推薦論文リスト</a>」を消化すべきなのだろう．</p>
<h2 id="ネットワーク構造"><a href="#ネットワーク構造" class="headerlink" title="ネットワーク構造"></a>ネットワーク構造</h2><p>　パーセプトロンは小脳に，BESOMは大脳皮質に，畳み込みニューラルネットは受容野に，TD学習は大脳基底核に似ていると言われている．ではどこが．<br>　小脳についてはさきほど挙げた『<a href="http://www.amazon.co.jp/dp/478281514X/" target="_blank" rel="external">脳の計算理論</a>』がよい．この<a href="http://www.cns.atr.jp/~kawato/Japanese.html" target="_blank" rel="external">著者</a>は小脳による運動の内部モデル獲得というテーマの大家だそうで，公開されている無料のPDFでその足跡が追える．Hodgkin-Huxleyモデルの解説も丁寧．あわせて“<a href="http://ci.nii.ac.jp/naid/10028190905" target="_blank" rel="external">現代の小脳パーセプトロン仮説</a>”も読みたい．<br>　大脳皮質については<a href="https://staff.aist.go.jp/y-ichisugi/j-index.html" target="_blank" rel="external">BESOMの人</a>の「<a href="https://staff.aist.go.jp/y-ichisugi/rapid-memo/brain-deep-learning.html" target="_blank" rel="external">大脳皮質と deep learning の類似点と相違点</a>」がとにかくわかりやすい．やはり正則化が重要なようだが，それについては「<a href="http://blog.livedoor.jp/brain_network/archives/50968197.html" target="_blank" rel="external">脳とネットワーク/The Swingy Brain:まとめてスパースコーディング - livedoor Blog（ブログ）</a>」に挙げられている論文を読むとよさそう．<br>　さらなる部位との関連については，「<a href="https://staff.aist.go.jp/y-ichisugi/brain-archi/j-index.html" target="_blank" rel="external">全脳アーキテクチャ解明に向けて</a>」から辿れる資料が親切だった．特に「<a href="http://www.slideshare.net/sato0427/wba3rd-satonao" target="_blank" rel="external">海馬神経回路の機能ダイナミクス</a>」で触れられている内容は元論文の古さとは裏腹にあまり書籍では見ない．</p>
<h2 id="微分計算の手法"><a href="#微分計算の手法" class="headerlink" title="微分計算の手法"></a>微分計算の手法</h2><p>　みんな大好き誤差逆伝播法は脳では使われていない．じゃあどうすればいいかというと：</p>
<ul>
<li>『<a href="http://www.amazon.co.jp/dp/0262650541" target="_blank" rel="external">Computational Explorations in Cognitive Neuroscience: Understanding the Mind by Simulating the Brain</a>』<ul>
<li>5章</li>
</ul>
</li>
<li>“<a href="http://arxiv.org/abs/1412.7525" target="_blank" rel="external">Difference Target Propagation</a>”<ul>
<li><a href="http://deeplearning.jp/wp-content/uploads/2014/04/20150826_suzuki.pdf" target="_blank" rel="external">日本語の解説</a></li>
</ul>
</li>
<li>“<a href="http://arxiv.org/abs/1502.04156" target="_blank" rel="external">Towards Biologically Plausible Deep Learning</a>”</li>
</ul>
<p>がある．後者2つは深層学習の大家であるBengioらの研究で，ここでSTDPが重要となってくるらしい．生物学的な妥当な深層学習まであと何年だろうか．</p>
<h1 id="おわりに"><a href="#おわりに" class="headerlink" title="おわりに"></a>おわりに</h1><p>　機械学習のことは多少わかるけど（計算論的）神経科学については何から勉強すればいいのかもわからないというところから，騙し騙しとはいえ論文を読める程度になるにはこのあたりを読むとよいのではないかと思います．<br>　なお，このブログに貼られているAmazonリンクはいずれも素のリンクです．ご安心ください．アフィリエイトリンクを貼りまくって小銭を稼ごうと画策しましたが審査に落ちました．</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://ntddk.github.io/2016/03/21/book-guide-for-computational-neuroscience/" data-id="cjbiq70rz000h8w7tfhq7al9y" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/">machine learning</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2016/03/02/sushi-and-memorized-recursion/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          寿司とメモ化再帰
        
      </div>
    </a>
  
  
    <a href="/2016/03/26/neural-image-caption-generator-twitter-bot/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">写真から説明文を自動生成するbotを作った</div>
    </a>
  
</nav>

  
</article>

</section>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 ntddk<br>
      <a href="https://github.com/ntddk/hexo-theme-jathena" target="_blank">JAthena</a> by <a href="https://ntddk.github.io" target="_blank">ntddk</a> | Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>
  </div>
</body>
</html>